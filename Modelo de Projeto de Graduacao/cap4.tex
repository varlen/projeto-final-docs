\chapter{Implementação}
\label{cap4}

\paragraph{} Este capítulo dedica-se a aprofundar a discussão sobre a solução criada, suas premissas, restrições e motivações. O capitulo se encerra com o detalhamento da solução e de seus componentes.

\section{Requisitos e Limitações}

\paragraph{} A principal premissa a ser levada em consideração para implementação é que a estrutura dos dados deve ser conservada. Isso significa que as tabelas, as colunas, seus tipos e suas restrições relacionais, como chaves primárias e estrangeiras, deverão manter a compatibilidade com o banco de dados de entrada.

\paragraph{} Isso reduz a gama de técnicas de anonimização passiveis de utilização neste trabalho, pois algumas destas dependem de operações que alteram os tipos semânticos dos dados. Por exemplo, a já apresentada anteriormente, substituição de uma idade específica (27 anos) por uma faixa etária (20-30 anos) para atingir k-anonimização.

\paragraph{} Para colunas cujos valores são numéricos, este trabalho abre mão da fidelidade estatística por preferência a simplicidade de implementação. Colunas numéricas serão anonimizadas a partir da substituição por uma distribuição uniforme.

\paragraph{} Deste modo, a ferramenta de anonimização projetada ao longo deste trabalho visa prioritariamente cumprir o caso de uso em que um terceiro deve ter acesso à um banco de dados análogo, preenchido com dados gerados, com mesmo sentido semantico. Mantém-se um subconjunto da utilidade original enquanto os dados são dessensibilizados, assim como se é esperado de um sistema de anonimização.

\paragraph{} Ainda com intuito de simplificar a implementação, ao usar componentes cuja base é aprendizado de máquina, este trabalho se restringe a utilizar modelos prontos ao invés de treinar seus próprios modelos.

\paragraph{} Com a intenção de criar uma solução extensível e reutilizável, optou-se por desenvolvimento dentro das premissas adequadas para software livre, de código aberto e disponibilizado na plataforma GitHub.

\section{Tecnologias Utilizadas}

\paragraph{} O sistema implementado é capaz de gerar um banco de dados relacional anonimizado a partir de um banco de dados original, a partir da substituição dos dados reais por dados gerados, com mesmos tipos e estrutura.

\paragraph{} A escolha de tecnologias para implementação tomou como critério a existência de bibliotecas para realizar as manipulações de dados previstas, facilidade de reuso, disponibilidade de documentação pública e também a familiaridade do autor.

\paragraph{} Este trabalho utilizou Python como linguagem de programação principal tendo a biblioteca SQLAlchemy como provedor de acesso ao banco de dados para compatibilidade com multiplos Sistemas de Gerenciamento de Banco de Dados. No decorrer do processo de desenvolvimento, optou-se por restringir a implementação inicial existente em um único sistema de banco de dados relacional, especificamente Postgres.

\paragraph{} O código específico de banco de dados foi implementado de forma a definir pontos de extensão a partir de módulos. O usuário pode adicionar seus módulos implementando compatibilidade com outros sistemas de bancos de dados e definir qual módulo deverá ser utilizado ao executar o sistema.

\paragraph{} O módulo de banco de dados precisa incluir um conjunto de consultas pré-definidas. Essas consultas são utilizadas para extrair amostras e metadados do banco de dados originário e devem utilizar a capacidade de introspecção específica de cada implementação de banco de dados.

\section{Componentes}

\paragraph{} Os componentes a seguir são os blocos lógicos do software e representam abstrações de alto nível das partes do sistema, visando facilitar o entendimento da solução como um todo.

\paragraph{} Cada componente realiza operações encadeadas dentro de um fluxo único de transformação dos dados que se inicia com a leitura do banco de dados que se deseja disponibilizar de maneira a garantir a privacidade e termina com a população de um novo banco de dados. A seguir está a descrição ordenada de cada componente:

\subsection{Analisador de Banco de Dados}

\paragraph{} O analisador de banco de dados é o componente responsável pelo inicio do fluxo de processamento da aplicação, obtendo as informações sobre o banco de dados e seus metadados a serem armazenadas em uma estrutura de dados comum que será utilizada posteriormente.

\paragraph{} Essa estrutura de dados é serializada como um arquivo de texto no formato JSON e persistida em disco. Isso permite que um passo seguinte do fluxo de processamento possa reutilizar o resultado de uma execução prévia deste componente, removendo a necessidade de um novo acesso ao banco de dados de origem.

\paragraph{} O analisador de banco de dados é composto pelo núcleo, pelo plugin de banco de dados, pelo analisador de estrutura e pelo analisador de tipos semânticos.

\subsubsection{Núcleo}

\paragraph{} O núcleo é o subcomponente responsável por agregar os fluxos do programa de extração, orquestrando as chamadas entre os compoenentes para compor a estrutura de dados final.

\paragraph{} O núcleo é responsável por inicializar o fluxo e carregar um plugin de banco de dados.

\subsubsection{Plugin de Banco de Dados}

\paragraph{} O plugin de banco de dados é um módulo que contém as consultas para obtenção de metadados e amostras do banco de dados de entrada.

\paragraph{} Este componente pode ter várias implementações de acordo com o banco de dados de entrada. No escopo deste trabalho, apenas a implementação do plugin para Postgres foi criada.

\subsubsection{Analisador de Estrutura}

\paragraph{} É o subcomponente do analisador de banco de dados responsável por capturar o desenho do banco de dados original, trazendo para o fluxo de dados suas colunas com os respectivos tipos atômicos e tabelas, além de amostras e estatísticas dos dados, a partir da realização das consultas necessarias para obtenção destas informações.

\paragraph{} O analisador de estrutura utilza as consultas fornecidas pelo plugin de banco de dados para obtenção de metadados.


\subsubsection{Analisador de Tipos Semânticos}

\paragraph{} O analisador de tipos semânticos é o componente responsável por obter o significado de uma coluna dentro do contexto das informações presentes no banco de dados a partir de suas amostras e é aplicado em colunas textuais para permitir a futura geração de dados semanticamente coerentes com o banco de dados original.

\paragraph{} Este processo pode ser feito a partir das técnicas previamente apresentadas como busca em dicionário e expressão regulares.

\paragraph{} Estas tecnicas, no entanto, apresentam limitações que as tornam inconvenientes para obtenção de alguns tipos semânticos como por exemplo o endereço.

\paragraph{} Como uma tentativa de contornar este problema e aumentar a abrangência da inferência de tipos semânticos, este componente utiliza uma solução externa criada a partir de técnicas de aprendizado de máquina, integrada a partir de chamadas HTTP.

\paragraph{} Para cada tabela do banco de origem, o analisador de tipos semânticos cria uma amostra no formato CSV contendo um subconjunto dos seus registros e a envia para a rota de classficação do serviço externo.

\paragraph{} Neste trabalho especificamente, utiliza-se uma solução chamada SATO\cite{zhang2019sato}.

\paragraph{} Um modelo pré-treinado chamado type78 é responsável pela classificação semântica dentro do SATO. Este modelo é capaz de classificar dos dados em 78 rótulos semânticos pré-definidos.

\paragraph{} Para compatibilidade, utilizou-se um container Docker com as dependências requeridas pelo SATO para fornecer um servidor HTTP que processa as requisições de classificação semantica.

\paragraph{} Desta forma, é possível inferir tipos semânticos de colunas em casos nos quais estes não poderiam ser obtidos com facilidade ou precisão.

\paragraph{} A classificação semântica de cada coluna pode então ser utilizada em etapas seguintes do fluxo para geração de dados análogos.

\subsubsection{Extrator Numérico}

\paragraph{} Para colunas de tipos numéricos, há um processamento que visa substituir o conjunto de dados originais com um conjunto cuja distribuição é análoga.

\paragraph{} Para isso, coletam-se amostras do conjunto original. Essas amostras são ordenadas. Um número aleatório é sorteado de uma distribuição uniforme entre 0 e 1 e multiplicado pelo número de amostras.

\paragraph{} A parte inteira deste número sorteado e escalado é utilizada como índice para acessar uma das amostras ordenadas do conjunto original.

\paragraph{} Realiza-se então a interpolação linear dessa amostra e da amostra seguinte, com pesos proporcionais a parte fracionária do número índice. Essa operação gera um novo número que é armazenado em um vetor.

\paragraph{} Esse processo é repetido até gerar um novo conjunto de números que caracteriza o conjunto numérico original. Quando ocorre a geração de dados, este novo conjunto passa pelo mesmo processo para ser expandido e gerar o conteúdo de uma coluna numérica anonimizada.

\paragraph{} Um estudo aprofundado visando discutir valores ótimos para o tamanho do conjunto caracterizador e também das alterações nas funções estatísticas está fora do escopo deste trabalho e será deixada para eventuais continuações.

\subsection{Gerador de Banco de Dados}

\paragraph{} O Gerador de Banco de Dados é o programa responsável pela geração da carga de saída do fluxo de processamento.

\paragraph{} Este programa requer como entradas o arquivo contendo as especificações geradas pelos componentes anteriores no formato JSON e também os parâmetros de conexão com o banco de dados de destino.

\paragraph{} O arquivo de especificações é utilizado para construção da estrutura e do conteúdo de um novo banco de dados pelos componentes do gerador.

\subsubsection{Construtor de Estrutura}

\paragraph{} O primeiro passo do fluxo de geração de dados é a criação das tabelas e suas relações, incluindo chaves primárias e extrangeiras.

\paragraph{} Para criação da estrutura do banco de dados, o arquivo de especificações é analisado. Chama-se então, a API do SQLAlchemy responsável por criar novas tabelas, suas colunas e chaves primárias com as informações de tabela e coluna especificadas.

\paragraph{} Em seguida, são criadas as relações de chave extrangeira de acordo com os metadados presentes no arquivo de estrutura.

\subsubsection{Gerador de Dados}

\paragraph{} Após a construção da estrutura do banco de dados de destino, inicia-se o processo de geração de dados. Este processo popula as novas tabelas com dados gerados de acordo com os tipos semânticos e distribuições inferidas na etapa de extração.

\paragraph{} Então, gera-se o conteúdo das tabelas. Para colunas que não possuem um elo com outra tabela, ou seja, que não são chaves estrangeiras, o dado é gerado utilizando um gerador selecionado a partir dos seus tipos atômico e semantico.

\paragraph{} Para geração de dados de acordo com o tipo semântico, utiliza-se um mapeamento do conjunto de rótulos inclusos no \textit{dataset} type78 para funções geradoras de dados. Essas funções geradoras são construídas a partir das capacidades de geração providas pela biblioteca Faker.

\paragraph{} Para geração de dados numéricos, cria-se um gerador de números utilizando o mesmo algoritmo utilizado no extrator. Esse algoritmo permite obter números cuja distribuição é análoga ao conjunto inicial.

\paragraph{} As colunas cujo dado é oriundo de outra tabela a partir de uma relação têm sua criação postergada até que a coluna de origem tenha dados gerados. Os dados gerados da origem são selecionados aleatóriamente para a coluna de destino.